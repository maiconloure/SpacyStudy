{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entendendo a tokenização\n",
    "doc = nlp(\"A galinha atravessou a rua em NY.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'galinha', 'atravessou', 'a', 'rua', 'em', 'NY', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ token.text for token in doc ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN \t A\n",
      "TOKEN \t galinha\n",
      "TOKEN \t atravessou\n",
      "TOKEN \t a\n",
      "TOKEN \t rua\n",
      "TOKEN \t em\n",
      "TOKEN \t NY\n",
      "SUFFIX \t .\n"
     ]
    }
   ],
   "source": [
    "# Debugando o tokenizador\n",
    "explain = nlp.tokenizer.explain(\"A galinha atravessou a rua em NY.\")\n",
    "for token in explain:\n",
    "  print(token[0], \"\\t\", token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack trouxe a peça para o Kawan.\n",
      "Kawan disse que queria relaxar.\n",
      "Jack disse que não entendeu.\n",
      "\"\n",
      "Como assim?\n",
      "Eu não entendi\", disse Jack.\n"
     ]
    }
   ],
   "source": [
    "# Segmentador de sentenças\n",
    "text = \"Jack trouxe a peça para o Kawan. Kawan disse que queria relaxar. Jack disse que não entendeu. \\\"Como assim? Eu não entendi\\\", disse Jack.\"\n",
    "doc = nlp(text)\n",
    "for sentence in doc.sents:\n",
    "    print(sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN \t Jack\n",
      "TOKEN \t trouxe\n",
      "TOKEN \t a\n",
      "TOKEN \t peça\n",
      "TOKEN \t para\n",
      "TOKEN \t o\n",
      "TOKEN \t Kawan\n",
      "SUFFIX \t .\n",
      "TOKEN \t Kawan\n",
      "TOKEN \t disse\n",
      "TOKEN \t que\n",
      "TOKEN \t queria\n",
      "TOKEN \t relaxar\n",
      "SUFFIX \t .\n",
      "TOKEN \t Jack\n",
      "TOKEN \t disse\n",
      "TOKEN \t que\n",
      "TOKEN \t não\n",
      "TOKEN \t entendeu\n",
      "SUFFIX \t .\n",
      "PREFIX \t \"\n",
      "TOKEN \t Como\n",
      "TOKEN \t assim\n",
      "SUFFIX \t ?\n",
      "TOKEN \t Eu\n",
      "TOKEN \t não\n",
      "TOKEN \t entendi\n",
      "SUFFIX \t \"\n",
      "SUFFIX \t ,\n",
      "TOKEN \t disse\n",
      "TOKEN \t Jack\n",
      "SUFFIX \t .\n"
     ]
    }
   ],
   "source": [
    "explain = nlp.tokenizer.explain(text)\n",
    "for token in explain:\n",
    "  print(token[0], \"\\t\", token[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1013006322a2339181c9f88d62f6a364bb1a779c7164bb035fe320876d0a330"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
